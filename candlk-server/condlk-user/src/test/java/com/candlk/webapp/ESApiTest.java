package com.candlk.webapp;

import java.io.IOException;
import java.util.*;

import co.elastic.clients.elasticsearch.ElasticsearchClient;
import co.elastic.clients.elasticsearch._types.SortOrder;
import co.elastic.clients.elasticsearch._types.mapping.Property;
import co.elastic.clients.elasticsearch._types.mapping.TypeMapping;
import co.elastic.clients.elasticsearch._types.query_dsl.Operator;
import co.elastic.clients.elasticsearch._types.query_dsl.TextQueryType;
import co.elastic.clients.elasticsearch.core.*;
import co.elastic.clients.elasticsearch.indices.GetMappingRequest;
import co.elastic.clients.elasticsearch.indices.PutMappingRequest;
import com.candlk.context.web.Jsons;
import com.candlk.webapp.es.ESEngineClient;
import com.candlk.webapp.user.entity.StopWord;
import com.candlk.webapp.user.entity.TweetWord;
import com.candlk.webapp.user.model.ESIndex;
import com.hankcs.hanlp.seg.common.Term;
import com.hankcs.hanlp.tokenizer.NLPTokenizer;
import com.hankcs.hanlp.tokenizer.NotionalTokenizer;
import lombok.extern.slf4j.Slf4j;
import me.codeplayer.util.CollectionUtil;
import me.codeplayer.util.StringUtil;
import org.junit.jupiter.api.BeforeAll;
import org.junit.jupiter.api.Test;

@Slf4j
public class ESApiTest {

	static ESEngineClient engine;

	@BeforeAll
	public static void init() throws IOException {
		engine = new ESEngineClient(
				"https://127.0.0.1:9200",
				// "f40da2681b97c673f3bf0c65e87d70d75ff166d405f6e36a024a96962df57c4d",
				// "elastic", "aS26ZiHC_U+sCJlDccoW"

				"008d29b8ca1324053f3cc196ae88e9e2dc865463053f0880f9c5cf708588f818",
				"elastic", "YqR6mL+USUyJnPYPKtG="
		);
	}

	@Test
	public void addKeyWords() throws Exception {
		final Date now = new Date();
		Set<String> wordSet = new HashSet<>(Arrays.asList(
		));
		List<String> words = wordSet.stream().toList();
		int size = words.size();
		List<TweetWord> keyWords = new ArrayList<>(size);
		final int offset = 0;
		for (int i = 0; i < size; i++) {
			TweetWord e = new TweetWord(words.get(i), 0, 0, 0L, now);
			e.setId(i + 1L + offset);
			keyWords.add(e);
		}
		engine.bulkAddDoc(ESIndex.KEYWORDS_INDEX, keyWords);
	}

	@Test
	public void addStopWords() throws Exception {
		final Date now = new Date();
		// ç¤ºä¾‹ï¼šæ‰¹é‡æ·»åŠ åœç”¨è¯
		List<StopWord> stopWords = Arrays.asList(
				new StopWord("çš„", now),
				new StopWord("the", now),
				new StopWord("æ˜¯", now),
				new StopWord("@", now),
				new StopWord("$", now)
		);
		for (int i = 0; i < stopWords.size(); i++) {
			stopWords.get(i).setId(i + 1L);
		}
		engine.bulkAddDoc(ESIndex.STOP_WORDS_INDEX, stopWords);
		// éªŒè¯åœç”¨è¯ç¼“å­˜
		System.out.println("åœç”¨è¯ç¼“å­˜: " + engine.stopWordsCache);
	}

	@Test
	public void searchKeywords() throws Exception {
		// ç¤ºä¾‹ï¼šæŸ¥è¯¢å…³é”®è¯ï¼ˆç¬¬ 1 é¡µï¼Œæ¯é¡µ 2 æ¡ï¼ŒæŒ‰ priority é™åºï¼‰
		List<TweetWord> results = engine.searchKeywords(ESIndex.KEYWORDS_INDEX, TweetWord.class,
				1, 6, TweetWord.UPDATE_TIME, SortOrder.Desc);
		log.info("æŸ¥è¯¢å…³é”®è¯: {}", Jsons.encode(results));
	}

	@Test
	public void delKeywords() throws Exception {
		engine.client.delete(d -> d.index("products").id("bk-1"));
	}

	@Test
	public void hanLpSegmentTest() {
		final String inputStr = "BlockBeats æ¶ˆæ¯ï¼Œ4 æœˆ 30 æ—¥ï¼Œæ® Cointelegraph æŠ¥é“ï¼ŒLedger ç¡¬ä»¶é’±åŒ…ç”¨æˆ·æ”¶åˆ°ä¼ªè£…æˆå®˜æ–¹çš„è¯ˆéª—ä¿¡ä»¶ï¼Œä¿¡ä¸­ä»¥ã€Œç´§æ€¥å®‰å…¨æ›´æ–°ã€ä¸ºç”±ï¼Œè¦æ±‚ç”¨æˆ·æ‰«æäºŒç»´ç å¹¶æä¾› 24 å­—ç§å¯†æ¢å¤çŸ­è¯­ï¼Œä»¥çªƒå–é’±åŒ…æ§åˆ¶æƒã€‚æŠ€æœ¯è¯„è®ºå‘˜ Jacob Canfield åœ¨ X å¹³å°æ›å…‰æ­¤ä¿¡ä»¶ï¼Œä¿¡ä»¶ç›—ç”¨ Ledger æ ‡å¿—ã€åœ°å€å’Œå‚è€ƒç¼–å·ï¼Œå¨èƒä¸éªŒè¯å°†é™åˆ¶é’±åŒ…è®¿é—®ã€‚\n"
				+ "\n"
				+ "\n"
				+ "\n"
				+ "Ledger å®˜æ–¹å›åº”ç§°æ­¤ä¸ºè¯ˆéª—ï¼Œå¼ºè°ƒä¸ä¼šé€šè¿‡ç”µè¯ã€æ¶ˆæ¯æˆ–è¦æ±‚æä¾›æ¢å¤çŸ­è¯­ï¼Œå‘¼åç”¨æˆ·è­¦æƒ•é’“é±¼æ”»å‡»ã€‚è¯ˆéª—æˆ–ä¸ 2020 å¹´ 7 æœˆ Ledger æ•°æ®æ³„éœ²æœ‰å…³ï¼Œå½“æ—¶è¶… 27 ä¸‡ç”¨æˆ·ä¸ªäººä¿¡æ¯ï¼ˆåŒ…æ‹¬å§“åã€ç”µè¯å’Œåœ°å€ï¼‰è¢«é»‘å®¢æ³„éœ²ã€‚";
		//æ ‡å‡†åˆ†è¯
		// List<Term> termList = StandardTokenizer.segment(inputStr);
		// System.out.println(termList);
		// //æ ‡å‡†åˆ†è¯å°è£…
		// System.out.println(HanLP.segment(inputStr));
		// NLPåˆ†è¯ è¯æ€§æ ‡æ³¨å’Œå‘½åå®ä½“è¯†åˆ«
		List<Term> segment = NLPTokenizer.segment(inputStr);
		System.out.println("NLPåˆ†è¯ç»“æœï¼š" + StringUtil.join(segment, term -> term.word, " | "));

		System.out.println("å®è¯åˆ†è¯å™¨ï¼š" + StringUtil.join(NotionalTokenizer.segment(inputStr), term -> term.word, " | "));

	}

	@Test
	public void hanLpEnSegmentTest() {
		final String inputStr = "\uD83D\uDE80 Excited to power this crucial discussion at #HackSeasonsConference in Dubai! \uD83D\uDD25 \n"
				+ "\n"
				+ "Join @0xPickleCati, @decatanomics, and @MidnightCryptoG as we dive deep into this topic. \n"
				+ "\n"
				+ "Register now:";
		// æ ‡å‡†åˆ†è¯
		// List<Term> termList = StandardTokenizer.segment(inputStr);
		// System.out.println("æ ‡å‡†åˆ†è¯ç»“æœï¼š" + StringUtil.join(termList, term -> term.word, " | "));
		// //æ ‡å‡†åˆ†è¯å°è£…
		// System.out.println("æ ‡å‡†åˆ†è¯å°è£…ï¼š" + StringUtil.join(HanLP.segment(inputStr), term -> term.word, " | "));
		// // NLPåˆ†è¯ è¯æ€§æ ‡æ³¨å’Œå‘½åå®ä½“è¯†åˆ«
		// List<Term> segment = NLPTokenizer.segment(inputStr);
		// System.out.println("NLPåˆ†è¯ç»“æœï¼š" + StringUtil.join(segment, term -> term.word, " | "));

		System.out.println("å®è¯åˆ†è¯å™¨ï¼š" + StringUtil.join(NotionalTokenizer.segment(inputStr), term -> term.word, " | "));

	}

	@Test
	public void tokenizerTest() throws IOException {
		final String inputStr = "\uD83C\uDF1E2ä¸ªèªæ˜é’±æ­£åœ¨ä¹°å®ƒï¼\uD83C\uDF1E\n"
				+ "\uD83C\uDF1E2 smart traders are buying it! \uD83C\uDF1E\n"
				+ "\n"
				+ "\uD83D\uDC8EToken: Longcoin (MC: $4.82K)\n"
				+ "\n"
				+ "Di6SRTDraS7L17UTPHMq8han1n4XBzDTcZXgi8yDpump\n"
				+ "\n"
				+ "æŸ¥çœ‹æˆ‘ä¸»é¡µç®€ä»‹å³å¯åŠ å…¥æ— å»¶è¿Ÿç¾¤ç»„   \n"
				+ "  View my homepage profile to join the no delay group";
		// åˆ†è¯
		List<Term> segment = NotionalTokenizer.segment(inputStr);
		Set<String> words = CollectionUtil.toSet(segment, term -> term.word);
		log.info("åˆ†è¯ç»“æœï¼š{}", StringUtil.joins(words, " | "));

		if (!words.isEmpty()) {
			SearchResponse<TweetWord> response = engine.client.search(s -> {
				s.index(ESIndex.KEYWORDS_INDEX.value);

				// terms æŸ¥è¯¢åŒ¹é… words å­—æ®µ
				// s.query(q -> q.terms(t -> t.field(TweetWord.WORDS).terms(tq -> tq.value(
				// 		words.stream().map(FieldValue::of).collect(Collectors.toList())
				// ))));

				// å¤šå­—æ®µæ¨¡ç³ŠåŒ¹é…æŸ¥è¯¢ï¼ˆmulti_match or shouldï¼‰
				s.query(q -> q.bool(b -> b.should(
						q1 -> q1.multiMatch(mm -> mm
								.query(inputStr) // åŸå§‹æ–‡æœ¬æˆ–æå–å…³é”®è¯çš„å­—ç¬¦ä¸²
								.fields("words.zh", "words.en", "words.fr", "words.es", "words.ja", "words.ko")
								.type(TextQueryType.BestFields) // å¯é€‰ï¼šä¹Ÿå¯ç”¨ most_fields æˆ– cross_fields
								.operator(Operator.Or)
						)
				)));

				// æ’åºè§„åˆ™ï¼štype(desc) > priority(desc) > updateTime(desc)
				s.sort(so -> so.field(f -> f.field(TweetWord.TYPE).order(SortOrder.Asc)));
				s.sort(so -> so.field(f -> f.field(TweetWord.COUNT).order(SortOrder.Desc)));

				// è®¾ç½®æœ€å¤§è¿”å›æ•°é‡ï¼Œå®é™…ä¸šåŠ¡ä¸­åº”åˆ†é¡µ
				s.size(30);
				return s;
			}, TweetWord.class);
			List<TweetWord> tweetWords = ESEngineClient.toT(response);
			// åŒ¹é… ES ä¸­çš„å…³é”®è¯
			log.info("å‘½ä¸­å…³é”®è¯æ•°: {}", tweetWords.size());

			for (TweetWord keyword : tweetWords) {
				log.info("å…³é”®è¯: {}, ç±»å‹: {}, è®¡æ•°: {}, ä¼˜å…ˆçº§: {}, æ›´æ–°æ—¶é—´: {}", keyword.getWords(), keyword.getType(), keyword.getCount(), keyword.getPriority(), keyword.getUpdateTime());
			}

		}
	}

	@Test
	public void testAddField() throws Exception {
		ElasticsearchClient client = engine.client;
		final String indexName = ESIndex.KEYWORDS_ACCURATE_INDEX.name().toLowerCase();
		final String fieldName = "status";
		// Step 1: è·å–ç´¢å¼•æ˜ å°„å…³ç³»
		final TypeMapping typeMapping = client.indices()
				.getMapping(GetMappingRequest.of(r -> r.index(indexName)))
				.get(indexName).mappings();
		final Map<String, Property> properties = typeMapping.properties();

		if (!properties.containsKey(fieldName)) {
			// Step 2: æ·»åŠ å­—æ®µ
			client.indices().putMapping(PutMappingRequest.of(req -> req
					.index(indexName)
					.properties(fieldName, Property.of(p -> p.integer(i -> i))) // TODO è®¾ç½®ä¸ºintegerç±»å‹ï¼
			));

			System.out.println("âœ… æ·»åŠ å­—æ®µæˆåŠŸï¼š" + fieldName);
		} else {
			System.out.println("â„¹ï¸ å­—æ®µå·²å­˜åœ¨ï¼š" + fieldName);
		}

		// Step 3: æ›´æ–°ç°æœ‰æ–‡æ¡£é»˜è®¤å€¼
		final UpdateByQueryResponse response = client.updateByQuery(UpdateByQueryRequest.of(req -> req
				.index(indexName)
				.script(s -> s
						// é€šè¿‡è„šæœ¬è®¾ç½®é»˜è®¤å€¼
						.source(builder -> builder.scriptString("if (ctx._source.status == null) { ctx._source.status = 1; }"))
						.lang("painless")
				)
				.query(q -> q
						.bool(b -> b
								.mustNot(mn -> mn
										.exists(e -> e.field(fieldName))
								)
						)
				)
		));
		System.out.println("ğŸ” æ›´æ–°æ–‡æ¡£æ•°æ®: " + response.updated());
	}

}
